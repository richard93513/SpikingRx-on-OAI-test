# 11/11 Report Log
Subject: SpikingRx Model Module Development Progress (LIF → Conv → Norm → SEW Block)

---

## 1. LIF Module (lif_neuron.py)

### Module Description
LIF (Leaky Integrate-and-Fire) is the core neuron module in the SpikingRx architecture.
Its function is to simulate biological neuronal temporal dynamics and convert continuous-valued inputs
(from Conv + Norm) into a time-series spike sequence (0/1).

### Position in SEW Block
Conv → Norm → **LIF** → Shortcut

---

## Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Key Code Components (with Corresponding Behavior)

| Code Block | Function / Mathematical Behavior | Key Concept |
|-----------|----------------------------------|-------------|
| class TriangularSurrogate | Custom autograd function | Creates a differentiable approximation of the spike function |
| forward() → (x >= 0).to(x.dtype) | Heaviside step: output 1 when membrane potential exceeds threshold | Simulates firing condition |
| backward() → grad = grad_input * (1 - x.abs() / a) * mask / a | Surrogate gradient: passes gradient only near threshold | Prevents gradient explosion or vanishing |
| LIF.forward() → U = β*U + (1-β)*I[t] | Integration phase | Accumulates input current into membrane potential |
| S = spike_fn(U - θ) | Threshold check (Fire) | Outputs a spike if crossing threshold |
| U = U - S*θ | Reset after firing | Resets membrane potential after spike |
| torch.stack(out_spikes, dim=1) | Forms spike train [B,T,C,H,W] | Produces time-series spikes |

---

## Test Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Test Results
![IMAGE PLACEHOLDER]()

### (1) Text Results
Input dimension: [B, T, D] = [1, 20, 1], representing one neuron over 20 time steps.

Output:
Spikes over time: tensor([... 1., ..., 1., ..., 1.])  
Total spikes: **3**

Meaning:
- Neuron fired 3 times across 20 steps.
- Confirms correct functioning of:
  - Integration
  - Threshold check
  - Reset after firing
- LIF dynamically produces spikes based on input magnitude.

---

### (2) Image Interpretation
![IMAGE PLACEHOLDER]()

- Gray line: input I[t]
- Blue line: membrane potential U[t]
- Red line: threshold θ = 0.5
- Orange dots: firing events

Observations:
- Membrane potential accumulates and crosses threshold → neuron fires.
- After firing, U resets downward → sawtooth pattern.
- Exactly 3 firing events match the text output.

This validates the correct Integrate → Fire → Reset procedure.

---

## Concept Notes
- The LIF firing cycle is three steps: Integrate → Fire → Reset.
- All-zero initial outputs occur when input never crosses θ.
- LIF has **no weights**; only hyperparameters β, θ.
- Surrogate gradient only affects **backward pass**, forward output remains binary.
- Output spike train goes to next SEW block.

---

# 2. Conv Module (conv_block.py)

## Module Description
ConvBlock extracts local spatial structure from frequency-domain feature maps.
It is:
- the **first layer** of the SEW block  
- the **only module with trainable weights**

### Position in SEW Block
**Conv** → Norm → LIF → Shortcut

---

## Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Key Components

| Code Block | Function / Math Behavior | Key Concept |
|-----------|---------------------------|-------------|
| self.conv = nn.Conv2d(...) | Defines convolution kernel | Corresponds to Y = W * X + b |
| nn.init.kaiming_normal_(...) | Weight initialization | Suitable for LIF/ReLU-like activations |
| forward() → self.conv(x) | Forward convolution | Produces feature map [B,C_out,H,W] |

---

## Test Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Test Results
![IMAGE PLACEHOLDER]()

### (1) Text Results
- Input channels: 3  
- Output channels: 8  
- Spatial size remains 32×32  

Output stats:
- Mean ≈ 0  
- Std ≈ 0.9  

Interpretation:
- Features stable
- Kaiming initialization functioning
- Forward pass successful

---

### (2) Image Results

![IMAGE PLACEHOLDER]()

**Left Image — Conv Filters**
- Each square is one 3×3 kernel
- Colormap bwr (red = positive, blue = negative)
- Rows correspond to same filter across input channels (f0-ch0, f0-ch1, f0-ch2)
- Mix of symmetry + randomness → good diverse sensitivity

**Right Image — Feature Maps**
- 8 maps (Map 0~7), each 32×32
- Color intensity = activation strength
- Even distribution → no collapse

---

## Concept Notes
- ConvBlock has **no activation**; LIF provides nonlinearity.
- padding=1 preserves spatial alignment with shortcut.
- Visualizing filters helps understand sensitivity.
- Tests overwrite previous images to only keep newest.

---

# 3. Norm Module (norm_layer.py)

## Module Description
SpikeNorm normalizes convolution output to stabilize membrane potential before LIF.

Key difference from BatchNorm:
- **Does not average across time**
- Preserves temporal sparsity

### Position in SEW Block
Conv → **Norm** → LIF → Shortcut

---

## Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Key Components

| Code Block | Function / Behavior | Key Concept |
|-----------|----------------------|-------------|
| self.bn = nn.BatchNorm2d(C) | Defines normalization per channel | Normalizes spatial distribution |
| forward(): dimension check | Supports [B,C,H,W] and [B,T,C,H,W] | Handles time-series inputs |
| for each t: self.bn(x[:,t]) | Normalizes each time step independently | Keeps temporal independence |
| stack along dim=1 | Reconstructs [B,T,C,H,W] | Forms full time series |

---

## Test Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Test Results
![IMAGE PLACEHOLDER]()

### (1) Text Results
SpikeNorm normalizes data to:
- mean = 0  
- std = 1  

Works for single-step and time-series inputs.

Meaning:
- Temporal independence preserved  
- No drift  
- Stable inputs for LIF

---

### (2) Histogram Results
![IMAGE PLACEHOLDER]()

Observations:
- Before/after histograms overlap near 0
- Range ≈ ±4
- Distribution becomes more concentrated and stable

Interpretation:
- SpikeNorm ensures consistent scaling before LIF
- Helps LIF threshold fire properly

---
# 4. SEW Block Module (sew_block.py)

## Module Description
SEW (Spike-Element-Wise) Block is the main structural unit of SpikingRx.
It integrates convolution, normalization, LIF spiking, and residual (shortcut) connections.
This corresponds to the SEW-ResNet block in the paper.

![IMAGE PLACEHOLDER]()

## Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

## Key Components (with Corresponding Behavior)

Code Block | Function / Mathematical Behavior | Key Concept
---------- | -------------------------------- | -----------
self.conv_block = ConvBlock(...) | Main branch convolution | Extracts primary features
self.norm = SpikeNorm(...) | Normalization | Stabilizes activation scale
self.lif = LIF(...) | LIF spiking | Converts features to spikes
if in_ch != out_ch: shortcut = Conv2d(1x1) | Channel alignment | Ensures equal dimensions for addition
y = main + shortcut | Residual fusion | Gradient splits automatically

## Test Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

## Test Results
![IMAGE PLACEHOLDER]()

(1) Text Results

Two scenarios were tested:

1. Identity Shortcut (input and output channels equal)
Input shape: [2, 5, 4, 32, 32]
Output shape: [2, 5, 4, 32, 32]
Mean: 0.0023
Std: 1.0016
Interpretation: Dimensions match; output stable.

2. Convolution Shortcut (input and output channels differ)
Input shape: [2, 5, 4, 32, 32]
Output shape: [2, 5, 8, 32, 32]
Mean: 0.0247
Std: 0.5712
Interpretation: 1x1 convolution aligns channels; output is reasonable.

Spike activity over time:
tensor([-0.0057, -0.0166, 0.0308, 0.0088, -0.0061])

This shows temporal stability and sparse firing.

(2) Image Results
![IMAGE PLACEHOLDER]()

A line plot shows mean spike activity per time step:
- Higher activity in steps 2 and 3.
- Near-zero in other steps.
This indicates temporal sparsity and stability.

## Concept Notes

Why residual addition does not mix incorrectly:
PyTorch autograd's AddBackward node divides the gradient into two paths:

grad_output -> main
grad_output -> shortcut

This allows both paths to receive gradients properly.

Shortcut benefits:
- Stabilizes gradient flow.
- Preserves original representation.
- Helps training convergence.

Monitoring recommendations:
- Output mean/std ideally around 0.6 to 1.0
- Spike activity oscillation around +-0.005
- Channel variance indicates shortcut effectiveness

# 5. Current Progress Summary

Module | Status | Function
------ | ------ | --------
lif_neuron.py | Completed | Temporal spiking core
conv_block.py | Completed | Feature extraction
norm_layer.py | Completed | Time-independent normalization
sew_block.py | Completed | Residual and spiking integration

All core SEW-ResNet front-end modules are fully implemented and validated.

# 6. Overall Understanding and Next Steps

The front half of the SpikingRx model is complete:
Conv -> Norm -> LIF -> Shortcut (repeated across SEW blocks)

All modules align with the SpikingRx paper:
- Convolution uses Kaiming initialization
- SpikeNorm stabilizes membrane potentials
- LIF implements temporal dynamics with Integrate-Fire-Reset
- SEW blocks fuse residual and main paths

# Recommended Next Steps

1. Implement spikingrx_model.py
   - Stack multiple SEW blocks
   - Add ANN readout to generate LLR output

2. Build higher-level test (test_spikingrx.py)
   - Full-model forward pass verification

3. Analytical suggestions
   - Analyze spike density and temporal distribution
   - Evaluate effect of beta and theta on firing rate
   - Visualize spike activity per layer
