# 11/11 Report Log
Subject: SpikingRx Model Module Development Progress (LIF → Conv → Norm → SEW Block)

---

## 1. LIF Module (lif_neuron.py)

### Module Description
LIF (Leaky Integrate-and-Fire) is the core neuron module in the SpikingRx architecture.
Its function is to simulate biological neuronal temporal dynamics and convert continuous-valued inputs
(from Conv + Norm) into a time-series spike sequence (0/1).

### Position in SEW Block
Conv → Norm → **LIF** → Shortcut

---

## Code

```python
# src/models/lif_neuron.py

# --------------------------------------------------------
# 模組位置對應：SpikingRx → SEW-ResNet block → LIF 神經元
# --------------------------------------------------------
# 這個檔案 lif_neuron.py 對應 SpikingRx 論文架構中的「spiking activation 層」
# 在 SEW-ResNet block 中的流程為：
# Conv → Norm → LIF(spike產生) → Shortcut → 下一層
# LIF 是在這裡把連續值 feature 轉成 0/1 的 spike。
# --------------------------------------------------------

import torch                    # 匯入 PyTorch 主函式庫
import torch.nn as nn           # 匯入神經網路模組 (nn.Module, Parameter)
# --------------------------------------------------------
# SpikingRx: 實際訓練與推論都是基於 PyTorch 實作的 LIF 模型
# --------------------------------------------------------

# ========================================================
# 一、Triangular Surrogate Gradient（三角形替代梯度）
# ========================================================
# 這是論文中明確使用的 surrogate gradient：
# σ′(x) = max(0, 1 - |x|/a) / a
# 只在 |x| < a 範圍內有梯度，其他地方為 0。
# 好處：避免梯度爆炸，且只在閾值附近更新。
# ========================================================

class TriangularSurrogate(torch.autograd.Function):        # 定義自訂 autograd 函式（前向+反向）
    @staticmethod
    def forward(ctx, x, a=1.0):                            # forward: 前向傳遞，ctx 可用來存值
        ctx.save_for_backward(x)                           # 把 x（膜電位減閾值）存起來，反向時用
        ctx.a = a                                          # 把參數 a 存起來（控制梯度範圍）
        return (x >= 0).to(x.dtype)                        # Heaviside step：若 U≥θ 輸出1，否則0
        # ✳️ 這一步對應 SpikingRx 的「S[t] = H(U[t] - θ)」
        # ✳️ forward 回傳的就是這個時間步的 spike（二值 0/1）

    @staticmethod
    def backward(ctx, grad_output):                        # backward: 反向傳遞時呼叫
        (x,) = ctx.saved_tensors                           # 取出 forward 時存的 x
        a = ctx.a                                          # 取出平滑控制參數 a
        grad_input = grad_output.clone()                   # 複製上游梯度 (∂L/∂S)

        # Triangular surrogate 公式：σ′(x) = max(0, 1 - |x|/a) / a
        mask = (x.abs() < a).to(x.dtype)                   # 只在 |x| < a 的範圍內保留梯度
        grad = grad_input * (1 - x.abs() / a) * mask / a   # 計算替代梯度（在閾值附近非零）
        # ✳️ 這裡就是 surrogate gradient 的精髓：
        #    spike 雖然不可微，但這裡給它一個「倒V形」的假梯度讓它能學。
        return grad, None                                  # 回傳對應於 (x, a) 的梯度（a不需要梯度）

# --------------------------------------------------------
# spike_fn 是包裝好的方便函式
# 讓外部直接呼叫 spike_fn(U - θ) 產生 spike。
# --------------------------------------------------------
def spike_fn(x, a=1.0):
    return TriangularSurrogate.apply(x, a)                 # 使用上面定義的自訂 autograd function


# ========================================================
# 二、LIF 模型 (Leaky Integrate-and-Fire Neuron)
# ========================================================
# 在 SpikingRx 的 SEW-ResNet block 中，
# LIF 是「spiking activation」：
#   Conv 輸出連續值特徵 → 經 Norm 正規化 → 傳入 LIF
#   LIF 會隨時間步 t 累積膜電位，超過閾值 θ 時發出 spike。
# ========================================================

class LIF(nn.Module):                                      # 定義一個神經元模組
    """
    符合 SpikingRx 論文的離散時間 LIF 模型：
      U[t] = βU[t−1] + (1−β)I[t]             ← 膜電位積分（leaky）
      S[t] = spike_fn(U[t] − θ)               ← 判斷是否跨閾產生spike
      U[t] = U[t] − S[t]θ                     ← 發泡後 soft reset
    """

    def __init__(self, beta=0.9, theta=1.0, learn_beta=False):
        super().__init__()                    # 初始化父類別
        if learn_beta:
            # 若 learn_beta=True，讓 β 成為可訓練參數（但論文中固定β=0.9）
            self.beta = nn.Parameter(torch.tensor(float(beta)))
        else:
            # 否則註冊成 buffer（參數固定，但會隨 model.to(device) 一起移動）
            self.register_buffer("beta", torch.tensor(float(beta)))

        # 閾值 θ：當膜電位超過此值時產生spike
        self.register_buffer("theta", torch.tensor(float(theta)))

    # ----------------------------------------------------
    # forward: LIF 的主要時序運算
    # I: [B, T, C, H, W] （卷積輸出）或 [B, T, D]（全連接輸出）
    # 對應 SpikingRx 論文中：每一個 block 都在 T 個時間步上運作。
    # ----------------------------------------------------
    def forward(self, I):
        if I.dim() == 5:                                     # [B, T, C, H, W]
            B, T, C, H, W = I.shape
            U = I.new_zeros((B, C, H, W))                    # 初始膜電位 U[0]=0
            out_spikes = []                                  # 用來收集每個時間步的 spike
            for t in range(T):                               # 時間展開：模擬 T 個時間步
                It = I[:, t]                                 # 取第 t 個時間步的輸入
                U = self.beta * U + (1 - self.beta) * It     # 更新膜電位（leaky integration）
                # ✳️ 對應論文公式：U[t] = βU[t−1] + (1−β)I[t]

                S = spike_fn(U - self.theta)                 # 判斷是否放電（跨閾值）
                # ✳️ 對應論文：S[t] = H(U[t] − θ)
                # ✳️ forward 時這裡是 0/1，但 backward 時用 triangular surrogate 傳梯度

                U = U - S * self.theta                       # 放電後 soft reset
                # ✳️ 對應論文：U[t] = U[t] − S[t]θ
                # ✳️ 若放電 S=1 → 減掉 θ；若沒放電 S=0 → 保留目前膜電位。

                out_spikes.append(S)                         # 把這個時間步的 spike 存起來

            return torch.stack(out_spikes, dim=1)            # 組成 spike train [B, T, C, H, W]
            # ✳️ SpikingRx：每層 LIF 都會輸出一串 spike train，傳給下一個 block。
            # ✳️ ANN 最後一層會整合所有時間步的 spike 來算 LLR。

        elif I.dim() == 3:                                   # [B, T, D] → 用於最後全連接層
            B, T, D = I.shape
            U = I.new_zeros((B, D))                          # 初始化膜電位
            out_spikes = []
            for t in range(T):
                It = I[:, t]                                 # 第 t 步輸入
                U = self.beta * U + (1 - self.beta) * It     # 更新膜電位
                S = spike_fn(U - self.theta)                 # 是否放電
                U = U - S * self.theta                       # 重設
                out_spikes.append(S)
            return torch.stack(out_spikes, dim=1)            # [B, T, D]

        else:
            raise ValueError("LIF input must be [B,T,C,H,W] or [B,T,D]")
            # ✳️ 確保輸入維度正確：SpikingRx 的輸入一定有時間步 T。

# --------------------------------------------------------
# 模組小結：
# - ArcTanSurrogate 改成 TriangularSurrogate → 與論文一致。
# - LIF 方程與 β, θ 完全對應論文。
# - forward() 的時間展開對應 SpikingRx 中的時序演化。
# - 最終輸出的 spike train 會進入下一個 SEW block 或 ANN 層。
# --------------------------------------------------------
```
---

## Key Code Components (with Corresponding Behavior)

| Code Block | Function / Mathematical Behavior | Key Concept |
|-----------|----------------------------------|-------------|
| class TriangularSurrogate | Custom autograd function | Creates a differentiable approximation of the spike function |
| forward() → (x >= 0).to(x.dtype) | Heaviside step: output 1 when membrane potential exceeds threshold | Simulates firing condition |
| backward() → grad = grad_input * (1 - x.abs() / a) * mask / a | Surrogate gradient: passes gradient only near threshold | Prevents gradient explosion or vanishing |
| LIF.forward() → U = β*U + (1-β)*I[t] | Integration phase | Accumulates input current into membrane potential |
| S = spike_fn(U - θ) | Threshold check (Fire) | Outputs a spike if crossing threshold |
| U = U - S*θ | Reset after firing | Resets membrane potential after spike |
| torch.stack(out_spikes, dim=1) | Forms spike train [B,T,C,H,W] | Produces time-series spikes |

---

## Test Code

```python
# src/tests/test_lif.py
import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../models")))

import torch
import matplotlib.pyplot as plt
from lif_neuron import LIF

# --------------------------------------------------------
# 建立 LIF 模組
# --------------------------------------------------------
lif = LIF(beta=0.9, theta=0.5)

# 一維特徵測試
B, T, D = 1, 20, 1
I = torch.ones(B, T, D) * 1.0 + 0.1 * torch.randn(B, T, D)

print("=== Test A: [B,T,D] ===")
print("Input shape:", I.shape)
out = lif(I)
print("Output shape:", out.shape)
print("Spikes over time:", out.view(T))
print("Total spikes:", int(out.sum().item()))

# --------------------------------------------------------
# 額外記錄膜電位 U[t]、輸入 I[t]、spike S[t] 並畫圖
# --------------------------------------------------------
U = torch.zeros(B, D)
U_list, S_list, I_list = [], [], []

for t in range(T):
    It = I[:, t]
    U = lif.beta * U + (1 - lif.beta) * It
    S = (U >= lif.theta).float()
    U = U - S * lif.theta

    I_list.append(It.item())
    U_list.append(U.item())
    S_list.append(S.item())

# --------------------------------------------------------
# 畫圖
# --------------------------------------------------------
time = list(range(1, T + 1))
fig, ax1 = plt.subplots(figsize=(8, 4))

ax1.plot(time, I_list, 'k--', label='Input I[t]', alpha=0.5)
ax1.plot(time, U_list, 'b-', label='Membrane Potential U[t]')
ax1.axhline(y=lif.theta.item(), color='r', linestyle=':', label='Threshold θ')
ax1.set_xlabel('Time step')
ax1.set_ylabel('U[t], I[t]')
ax1.legend(loc='upper left')

# Spike (用另一軸畫 0/1)
ax2 = ax1.twinx()
ax2.scatter(time, S_list, color='orange', label='Spike S[t]', marker='o')
ax2.set_ylabel('Spike (0 or 1)')
ax2.set_ylim(-0.1, 1.2)
ax2.legend(loc='upper right')

plt.title('LIF Neuron Dynamics')
plt.tight_layout()

# --------------------------------------------------------
# 儲存圖檔到 data/
# --------------------------------------------------------
save_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../data/lif_spike_plot.png"))
os.makedirs(os.path.dirname(save_path), exist_ok=True)
plt.savefig(save_path)
plt.close()

print(f"✅ 圖片已儲存到: {save_path}")



```
---

## Test Results
<img width="816" height="141" alt="image" src="https://github.com/user-attachments/assets/c85f2f2a-b03c-4421-b040-c3420024dd7f" />

<img width="531" height="265" alt="image" src="https://github.com/user-attachments/assets/3f2d7e75-d607-4a04-8fef-42e8fc2977d3" />

### (1) Text Results
Input dimension: [B, T, D] = [1, 20, 1], representing one neuron over 20 time steps.

Output:
Spikes over time: tensor([... 1., ..., 1., ..., 1.])  
Total spikes: **3**

Meaning:
- Neuron fired 3 times across 20 steps.
- Confirms correct functioning of:
  - Integration
  - Threshold check
  - Reset after firing
- LIF dynamically produces spikes based on input magnitude.

---

### (2) Image Interpretation
![IMAGE PLACEHOLDER]()

- Gray line: input I[t]
- Blue line: membrane potential U[t]
- Red line: threshold θ = 0.5
- Orange dots: firing events

Observations:
- Membrane potential accumulates and crosses threshold → neuron fires.
- After firing, U resets downward → sawtooth pattern.
- Exactly 3 firing events match the text output.

This validates the correct Integrate → Fire → Reset procedure.

---

## Concept Notes
- The LIF firing cycle is three steps: Integrate → Fire → Reset.
- All-zero initial outputs occur when input never crosses θ.
- LIF has **no weights**; only hyperparameters β, θ.
- Surrogate gradient only affects **backward pass**, forward output remains binary.
- Output spike train goes to next SEW block.

---

# 2. Conv Module (conv_block.py)

## Module Description
ConvBlock extracts local spatial structure from frequency-domain feature maps.
It is:
- the **first layer** of the SEW block  
- the **only module with trainable weights**

### Position in SEW Block
**Conv** → Norm → LIF → Shortcut

---

## Code
```python
# src/models/conv_block.py

# --------------------------------------------------------
# 模組位置對應：SpikingRx → SEW-ResNet block → Conv 卷積層
# --------------------------------------------------------
# 這個檔案 conv_block.py 對應 SpikingRx 論文架構中的「卷積前端」：
# 在每個 SEW-ResNet block 的起始位置：
#    Conv → Norm → LIF(spike) → Shortcut
# ConvBlock 的功能是萃取輸入特徵，並提供連續值給後續的 Norm 層。
# --------------------------------------------------------

import torch
import torch.nn as nn

# ========================================================
# 一、卷積模組 (ConvBlock)
# ========================================================
# 功能：
#   - 負責將輸入 feature map 做空間上的特徵提取。
#   - 輸出仍為連續值（非 spike），會交由下一層 SpikeNorm + LIF 處理。
#   - 權重使用 Kaiming Normal 初始化，以適配 ReLU-like 的 spiking 動態。
# ========================================================

class ConvBlock(nn.Module):
    """
    SpikingRx 卷積模組：
      對應 SEW-ResNet block 的第一層卷積 (Convolution layer)
      ConvBlock = Conv2d + Kaiming 初始化
      不包含 Norm 或啟動函數（Activation），
      因為這部分由後續的 SpikeNorm + LIF 負責。
    """

    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_bias=True):
        super(ConvBlock, self).__init__()  # 初始化父類別 (nn.Module)

        # ----------------------------------------------------
        # 定義卷積層：
        #   - in_channels:  輸入通道數
        #   - out_channels: 輸出通道數（feature map 數量）
        #   - kernel_size:  卷積核大小 (3x3)
        #   - stride:       步幅（預設1）
        #   - padding:      補零，確保輸出尺寸不變
        #   - bias:         是否使用偏置項
        # ----------------------------------------------------
        self.conv = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            bias=use_bias
        )

        # ----------------------------------------------------
        # 二、權重初始化 (Kaiming Normal)
        # ----------------------------------------------------
        # Kaiming Normal 是為 ReLU 類激活函數設計的初始化方法。
        # 在 SpikingRx 中，LIF 層的發火行為與 ReLU 類似（非線性截斷），
        # 因此使用相同原則可穩定梯度分佈。
        #   - mode='fan_out'：使輸出通道方差一致
        #   - nonlinearity='relu'：對應 spike 的啟動特性
        # ----------------------------------------------------
        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')

        # 若啟用 bias，將偏置初始化為 0
        if use_bias:
            nn.init.constant_(self.conv.bias, 0.0)

    # ----------------------------------------------------
    # 三、Forward 運算流程
    # ----------------------------------------------------
    # 輸入  : [B, C_in, H, W]
    # 輸出  : [B, C_out, H, W]
    # 說明  :
    #   ConvBlock 不包含 spike 運算，
    #   只負責在空間維度上卷積輸入特徵，
    #   並將輸出交由下一層 Norm + LIF 處理。
    # ----------------------------------------------------
    def forward(self, x):
        out = self.conv(x)  # 卷積運算
        return out          # 回傳連續特徵圖 (非二值化)

# --------------------------------------------------------
# 模組小結：
# - ConvBlock = 單純卷積 + Kaiming 初始化
# - 不包含 LIF 或正規化，後續由 SpikeNorm + LIF 處理
# - 對應 SEW-ResNet block 的第一步：「特徵提取」
# --------------------------------------------------------


```


---

## Key Components

| Code Block | Function / Math Behavior | Key Concept |
|-----------|---------------------------|-------------|
| self.conv = nn.Conv2d(...) | Defines convolution kernel | Corresponds to Y = W * X + b |
| nn.init.kaiming_normal_(...) | Weight initialization | Suitable for LIF/ReLU-like activations |
| forward() → self.conv(x) | Forward convolution | Produces feature map [B,C_out,H,W] |

---

## Test Code
```python
# src/tests/test_conv.py
import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../models")))

import torch
import matplotlib.pyplot as plt
from conv_block import ConvBlock

# --------------------------------------------------------
# 建立並測試 ConvBlock
# --------------------------------------------------------
conv = ConvBlock(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)
x = torch.randn(2, 3, 32, 32)   # 模擬輸入 [B,C,H,W]

print("=== ConvBlock Forward Test ===")
print("Input shape :", x.shape)
y = conv(x)
print("Output shape:", y.shape)
print(f"Output sample (mean,std): {y.mean().item():.4f}, {y.std().item():.4f}")

# --------------------------------------------------------
# 一、視覺化卷積核 (filters)
# --------------------------------------------------------
weights = conv.conv.weight.data.clone().detach().cpu()
out_channels, in_channels, kh, kw = weights.shape
print(f"Conv filters shape: {weights.shape}")

fig, axes = plt.subplots(out_channels, in_channels, figsize=(in_channels*2, out_channels*2))
fig.suptitle("Conv Filters", fontsize=14)

for i in range(out_channels):
    for j in range(in_channels):
        ax = axes[i, j] if out_channels > 1 else axes[j]
        w = weights[i, j].numpy()
        ax.imshow(w, cmap='bwr', interpolation='nearest')
        ax.axis('off')
        ax.set_title(f"f{i}-ch{j}")
plt.tight_layout()

save_path_filters = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../data/conv_filters.png"))
os.makedirs(os.path.dirname(save_path_filters), exist_ok=True)
plt.savefig(save_path_filters)
plt.close()
print(f"✅ 卷積核圖已儲存到: {save_path_filters}")

# --------------------------------------------------------
# 二、視覺化卷積後的特徵圖 (feature maps)
# --------------------------------------------------------
with torch.no_grad():
    feature_maps = conv(x)[0]  # 取第1筆輸出 [C,H,W]

fig, axes = plt.subplots(1, feature_maps.shape[0], figsize=(feature_maps.shape[0]*2, 2))
fig.suptitle("Output Feature Maps", fontsize=14)

for i, ax in enumerate(axes):
    fm = feature_maps[i].detach().cpu().numpy()
    ax.imshow(fm, cmap='viridis')
    ax.axis('off')
    ax.set_title(f"Map {i}")
plt.tight_layout()

save_path_fm = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../data/conv_featuremaps.png"))
plt.savefig(save_path_fm)
plt.close()
print(f"✅ 特徵圖已儲存到: {save_path_fm}")

```


---

## Test Results
<img width="561" height="155" alt="image" src="https://github.com/user-attachments/assets/87d06f51-d592-4730-8ac9-653952e9d82e" />
<img width="194" height="516" alt="image" src="https://github.com/user-attachments/assets/a0bb9993-b8e7-4406-a716-3a52481d1ad6" />
<img width="691" height="170" alt="image" src="https://github.com/user-attachments/assets/b798fd9b-b56f-4a1d-8e1c-d5a425ce59f1" /><img width="704" height="170" alt="image" src="https://github.com/user-attachments/assets/3e1d2eef-5aa0-45d9-850e-fc642ed136c4" />


### (1) Text Results
- Input channels: 3  
- Output channels: 8  
- Spatial size remains 32×32  

Output stats:
- Mean ≈ 0  
- Std ≈ 0.9  

Interpretation:
- Features stable
- Kaiming initialization functioning
- Forward pass successful

---

### (2) Image Results

![IMAGE PLACEHOLDER]()

**Left Image — Conv Filters**
- Each square is one 3×3 kernel
- Colormap bwr (red = positive, blue = negative)
- Rows correspond to same filter across input channels (f0-ch0, f0-ch1, f0-ch2)
- Mix of symmetry + randomness → good diverse sensitivity

**Right Image — Feature Maps**
- 8 maps (Map 0~7), each 32×32
- Color intensity = activation strength
- Even distribution → no collapse

---

## Concept Notes
- ConvBlock has **no activation**; LIF provides nonlinearity.
- padding=1 preserves spatial alignment with shortcut.
- Visualizing filters helps understand sensitivity.
- Tests overwrite previous images to only keep newest.

---

# 3. Norm Module (norm_layer.py)

## Module Description
SpikeNorm normalizes convolution output to stabilize membrane potential before LIF.

Key difference from BatchNorm:
- **Does not average across time**
- Preserves temporal sparsity

### Position in SEW Block
Conv → **Norm** → LIF → Shortcut

---

## Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Key Components

| Code Block | Function / Behavior | Key Concept |
|-----------|----------------------|-------------|
| self.bn = nn.BatchNorm2d(C) | Defines normalization per channel | Normalizes spatial distribution |
| forward(): dimension check | Supports [B,C,H,W] and [B,T,C,H,W] | Handles time-series inputs |
| for each t: self.bn(x[:,t]) | Normalizes each time step independently | Keeps temporal independence |
| stack along dim=1 | Reconstructs [B,T,C,H,W] | Forms full time series |

---

## Test Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Test Results
![IMAGE PLACEHOLDER]()

### (1) Text Results
SpikeNorm normalizes data to:
- mean = 0  
- std = 1  

Works for single-step and time-series inputs.

Meaning:
- Temporal independence preserved  
- No drift  
- Stable inputs for LIF

---

### (2) Histogram Results
![IMAGE PLACEHOLDER]()

Observations:
- Before/after histograms overlap near 0
- Range ≈ ±4
- Distribution becomes more concentrated and stable

Interpretation:
- SpikeNorm ensures consistent scaling before LIF
- Helps LIF threshold fire properly

---
# 4. SEW Block Module (sew_block.py)

## Module Description
SEW (Spike-Element-Wise) Block is the main structural unit of SpikingRx.
It integrates convolution, normalization, LIF spiking, and residual (shortcut) connections.
This corresponds to the SEW-ResNet block in the paper.

![IMAGE PLACEHOLDER]()

## Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

## Key Components (with Corresponding Behavior)

Code Block | Function / Mathematical Behavior | Key Concept
---------- | -------------------------------- | -----------
self.conv_block = ConvBlock(...) | Main branch convolution | Extracts primary features
self.norm = SpikeNorm(...) | Normalization | Stabilizes activation scale
self.lif = LIF(...) | LIF spiking | Converts features to spikes
if in_ch != out_ch: shortcut = Conv2d(1x1) | Channel alignment | Ensures equal dimensions for addition
y = main + shortcut | Residual fusion | Gradient splits automatically

## Test Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

## Test Results
![IMAGE PLACEHOLDER]()

(1) Text Results

Two scenarios were tested:

1. Identity Shortcut (input and output channels equal)
Input shape: [2, 5, 4, 32, 32]
Output shape: [2, 5, 4, 32, 32]
Mean: 0.0023
Std: 1.0016
Interpretation: Dimensions match; output stable.

2. Convolution Shortcut (input and output channels differ)
Input shape: [2, 5, 4, 32, 32]
Output shape: [2, 5, 8, 32, 32]
Mean: 0.0247
Std: 0.5712
Interpretation: 1x1 convolution aligns channels; output is reasonable.

Spike activity over time:
tensor([-0.0057, -0.0166, 0.0308, 0.0088, -0.0061])

This shows temporal stability and sparse firing.

(2) Image Results
![IMAGE PLACEHOLDER]()

A line plot shows mean spike activity per time step:
- Higher activity in steps 2 and 3.
- Near-zero in other steps.
This indicates temporal sparsity and stability.

## Concept Notes

Why residual addition does not mix incorrectly:
PyTorch autograd's AddBackward node divides the gradient into two paths:

grad_output -> main
grad_output -> shortcut

This allows both paths to receive gradients properly.

Shortcut benefits:
- Stabilizes gradient flow.
- Preserves original representation.
- Helps training convergence.

Monitoring recommendations:
- Output mean/std ideally around 0.6 to 1.0
- Spike activity oscillation around +-0.005
- Channel variance indicates shortcut effectiveness

# 5. Current Progress Summary

Module | Status | Function
------ | ------ | --------
lif_neuron.py | Completed | Temporal spiking core
conv_block.py | Completed | Feature extraction
norm_layer.py | Completed | Time-independent normalization
sew_block.py | Completed | Residual and spiking integration

All core SEW-ResNet front-end modules are fully implemented and validated.

# 6. Overall Understanding and Next Steps

The front half of the SpikingRx model is complete:
Conv -> Norm -> LIF -> Shortcut (repeated across SEW blocks)

All modules align with the SpikingRx paper:
- Convolution uses Kaiming initialization
- SpikeNorm stabilizes membrane potentials
- LIF implements temporal dynamics with Integrate-Fire-Reset
- SEW blocks fuse residual and main paths

# Recommended Next Steps

1. Implement spikingrx_model.py
   - Stack multiple SEW blocks
   - Add ANN readout to generate LLR output

2. Build higher-level test (test_spikingrx.py)
   - Full-model forward pass verification

3. Analytical suggestions
   - Analyze spike density and temporal distribution
   - Evaluate effect of beta and theta on firing rate
   - Visualize spike activity per layer
