# 11/11 Final Report Log
Subject: SpikingRx Model Module Development Progress (LIF → Conv → Norm → SEW Block)

---

## 1. LIF Module (lif_neuron.py)

### Module Description
LIF (Leaky Integrate-and-Fire) is the core neuron module in the SpikingRx architecture.
Its function is to simulate biological neuronal temporal dynamics and convert continuous-valued inputs
(from Conv + Norm) into a time-series spike sequence (0/1).

### Position in SEW Block
Conv → Norm → **LIF** → Shortcut

---

## Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Key Code Components (with Corresponding Behavior)

| Code Block | Function / Mathematical Behavior | Key Concept |
|-----------|----------------------------------|-------------|
| class TriangularSurrogate | Custom autograd function | Creates a differentiable approximation of the spike function |
| forward() → (x >= 0).to(x.dtype) | Heaviside step: output 1 when membrane potential exceeds threshold | Simulates firing condition |
| backward() → grad = grad_input * (1 - x.abs() / a) * mask / a | Surrogate gradient: passes gradient only near threshold | Prevents gradient explosion or vanishing |
| LIF.forward() → U = β*U + (1-β)*I[t] | Integration phase | Accumulates input current into membrane potential |
| S = spike_fn(U - θ) | Threshold check (Fire) | Outputs a spike if crossing threshold |
| U = U - S*θ | Reset after firing | Resets membrane potential after spike |
| torch.stack(out_spikes, dim=1) | Forms spike train [B,T,C,H,W] | Produces time-series spikes |

---

## Test Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Test Results
![IMAGE PLACEHOLDER]()

### (1) Text Results
Input dimension: [B, T, D] = [1, 20, 1], representing one neuron over 20 time steps.

Output:
Spikes over time: tensor([... 1., ..., 1., ..., 1.])  
Total spikes: **3**

Meaning:
- Neuron fired 3 times across 20 steps.
- Confirms correct functioning of:
  - Integration
  - Threshold check
  - Reset after firing
- LIF dynamically produces spikes based on input magnitude.

---

### (2) Image Interpretation
![IMAGE PLACEHOLDER]()

- Gray line: input I[t]
- Blue line: membrane potential U[t]
- Red line: threshold θ = 0.5
- Orange dots: firing events

Observations:
- Membrane potential accumulates and crosses threshold → neuron fires.
- After firing, U resets downward → sawtooth pattern.
- Exactly 3 firing events match the text output.

This validates the correct Integrate → Fire → Reset procedure.

---

## Concept Notes
- The LIF firing cycle is three steps: Integrate → Fire → Reset.
- All-zero initial outputs occur when input never crosses θ.
- LIF has **no weights**; only hyperparameters β, θ.
- Surrogate gradient only affects **backward pass**, forward output remains binary.
- Output spike train goes to next SEW block.

---

# 2. Conv Module (conv_block.py)

## Module Description
ConvBlock extracts local spatial structure from frequency-domain feature maps.
It is:
- the **first layer** of the SEW block  
- the **only module with trainable weights**

### Position in SEW Block
**Conv** → Norm → LIF → Shortcut

---

## Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Key Components

| Code Block | Function / Math Behavior | Key Concept |
|-----------|---------------------------|-------------|
| self.conv = nn.Conv2d(...) | Defines convolution kernel | Corresponds to Y = W * X + b |
| nn.init.kaiming_normal_(...) | Weight initialization | Suitable for LIF/ReLU-like activations |
| forward() → self.conv(x) | Forward convolution | Produces feature map [B,C_out,H,W] |

---

## Test Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Test Results
![IMAGE PLACEHOLDER]()

### (1) Text Results
- Input channels: 3  
- Output channels: 8  
- Spatial size remains 32×32  

Output stats:
- Mean ≈ 0  
- Std ≈ 0.9  

Interpretation:
- Features stable
- Kaiming initialization functioning
- Forward pass successful

---

### (2) Image Results

![IMAGE PLACEHOLDER]()

**Left Image — Conv Filters**
- Each square is one 3×3 kernel
- Colormap bwr (red = positive, blue = negative)
- Rows correspond to same filter across input channels (f0-ch0, f0-ch1, f0-ch2)
- Mix of symmetry + randomness → good diverse sensitivity

**Right Image — Feature Maps**
- 8 maps (Map 0~7), each 32×32
- Color intensity = activation strength
- Even distribution → no collapse

---

## Concept Notes
- ConvBlock has **no activation**; LIF provides nonlinearity.
- padding=1 preserves spatial alignment with shortcut.
- Visualizing filters helps understand sensitivity.
- Tests overwrite previous images to only keep newest.

---

# 3. Norm Module (norm_layer.py)

## Module Description
SpikeNorm normalizes convolution output to stabilize membrane potential before LIF.

Key difference from BatchNorm:
- **Does not average across time**
- Preserves temporal sparsity

### Position in SEW Block
Conv → **Norm** → LIF → Shortcut

---

## Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Key Components

| Code Block | Function / Behavior | Key Concept |
|-----------|----------------------|-------------|
| self.bn = nn.BatchNorm2d(C) | Defines normalization per channel | Normalizes spatial distribution |
| forward(): dimension check | Supports [B,C,H,W] and [B,T,C,H,W] | Handles time-series inputs |
| for each t: self.bn(x[:,t]) | Normalizes each time step independently | Keeps temporal independence |
| stack along dim=1 | Reconstructs [B,T,C,H,W] | Forms full time series |

---

## Test Code
![IMAGE PLACEHOLDER]()
![IMAGE PLACEHOLDER]()

---

## Test Results
![IMAGE PLACEHOLDER]()

### (1) Text Results
SpikeNorm normalizes data to:
- mean = 0  
- std = 1  

Works for single-step and time-series inputs.

Meaning:
- Temporal independence preserved  
- No drift  
- Stable inputs for LIF

---

### (2) Histogram Results
![IMAGE PLACEHOLDER]()

Observations:
- Before/after histograms overlap near 0
- Range ≈ ±4
- Distribution becomes more concentrated and stable

Interpretation:
- SpikeNorm ensures consistent scaling before LIF
- Helps LIF threshold fire properly

---

## Concept Notes
- Single test = spatial effect
- Multi-step = temporal independence
- randn() inputs look similar because they already mimic standard normal
- Mean/std tracking over time verifies stability
